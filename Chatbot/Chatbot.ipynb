{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBy_N3p4T_wz"
      },
      "source": [
        "**Initializing libiraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "27jhHZxtUIi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa12183-eb3d-403e-dcd8-5ecf0e0a8f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcxkIQt6UJbC"
      },
      "source": [
        "**Ask ChatGPT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwLY-yfZUOpE",
        "outputId": "4f6aa852-27d9-4201-a845-dc12d4bc8aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hello\n",
            "Chatbot: Hi! :D \n",
            "\n",
            "You: You are a chatbot, remember that\n",
            "Chatbot: I'm not a bot, I just like to talk. \n",
            "\n",
            "You: Ok\n",
            "Chatbot: I'm not sure if you're being serious or not, but I'll just say this : You have been banned from r pyongyang. \n",
            "\n",
            "You: what's that\n",
            "Chatbot: A bot that responds to you in the form of a message. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def load_chatbot():\n",
        "    model_name = \"microsoft/DialoGPT-medium\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    return tokenizer, model\n",
        "\n",
        "def chat():\n",
        "    tokenizer, model = load_chatbot()\n",
        "    chat_history_ids = None\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\").to(model.device)\n",
        "        chat_history_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
        "\n",
        "        response_ids = model.generate(\n",
        "            chat_history_ids,\n",
        "            max_length=chat_history_ids.shape[-1] + 50,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            temperature=0.6,\n",
        "            top_k=40,\n",
        "            top_p=0.85,\n",
        "            repetition_penalty=1.2,\n",
        "        )\n",
        "\n",
        "        print(\"Chatbot:\", tokenizer.decode(response_ids[:, chat_history_ids.shape[-1]:][0], skip_special_tokens=True), \"\\n\")\n",
        "\n",
        "chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}